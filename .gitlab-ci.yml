stages:
  - test-pipeline
  - build-pipeline
  - run-pipeline
  - test-api
  - build-api
  - deploy-api

# Change pip's cache directory to be inside the project directory since we can
# only cache local items.
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  HEROKU_STAGING_URL: https://$HEROKU_STAGING_APP.herokuapp.com/
  # tells pipenv where to keep virtualenvs
  WORKON_HOME: venv/
  # docker image details
  DOCKER_IMAGE: docker:20.10.7
  PYTHON_VERSION: 3.8.5
  PYTHON_IMAGE: python:${PYTHON_VERSION}-buster
  PIPELINE_IMAGE_NAME: password-pipeline
  PIPELINE_IMAGE_REF: $CI_REGISTRY_IMAGE/$PIPELINE_IMAGE_NAME:$CI_COMMIT_REF_SLUG
  PIPELINE_IMAGE_TAG: $CI_REGISTRY_IMAGE/$PIPELINE_IMAGE_NAME:$CI_COMMIT_SHA
  APP_IMAGE_NAME: password-app
  APP_IMAGE_REF: $CI_REGISTRY_IMAGE/$APP_IMAGE_NAME:$CI_COMMIT_REF_SLUG
  APP_IMAGE_TAG: $CI_REGISTRY_IMAGE/$APP_IMAGE_NAME:$CI_COMMIT_SHA

# Pip's cache doesn't store the python packages
# https://pip.pypa.io/en/stable/reference/pip_install/#caching
# If you want to also cache the installed packages, you have to install
# them in a virtualenv and cache it as well.
cache:
# untracked: true
 paths:
   - .cache/pip
   - venv/
   - .pyenv/cache

# === TEMPLATES DEFINITIONS ===

# define template which will execute jobs upon changes in ML pipeline
# because we unite package for ML pipeline and ML api,
# we need to carefully list here all files which changes shouldnt trigger CI/CD
# so it is a tradeoff of simplifying development at the cost of complicating CI/CD management
.pipeline-changes:
  only:
    changes:
      - packages/**/*
      - scripts/**/*
      - Dockerfile.pipeline
      - Pipfile
      - Pipfile.lock
      - config.toml
  except:
    refs:
      - tags

# define template which will execute jobs upon changes in ML pipeline except schedules
# (because Gitlab treat all files as changed upon schedule CI/CD execution)
.pipeline-build:
  extends: .pipeline-changes
  except:
    refs:
      - schedules
      - tags

# define template which will execute jobs upon changes in ML API
.api-changes:
  rules:
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH
      changes:
      - packages/**/*
      when: never
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH
      changes:
      - app/**/*
      - Dockerfile.app
    - if: '$CI_COMMIT_TAG != null'

# "extends" is able to merge hashes but not arrays, thus we need to repeat everything
# another option is to use .api-changes instead of .heroku-changes and .infra-changes
.heroku-changes:
  rules:
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH
      changes:
      - packages/**/*
      when: never
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH
      changes:
      - app/**/*
      - Dockerfile.app
      - .slugignore
      - Procfile
      - runtime.txt
    - if: '$CI_COMMIT_TAG != null'

# "extends" is able to merge hashes but not arrays, thus we need to repeat everything
.infra-changes:
  rules:
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH
      changes:
      - packages/**/*
      when: never
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH
      changes:
      - app/**/*
      - Dockerfile.app
      - config/**/*
      when: manual
      allow_failure: true
    - if: '$CI_COMMIT_TAG != null'
      when: manual
      allow_failure: true

# === JOBS DEFINITIONS ===

run-pytest-on-pipeline:
  extends: .pipeline-build
  stage: test-pipeline
  image: $PYTHON_IMAGE
  script:
    - echo

build-pipeline-docker-image:
  extends: .pipeline-build
  stage: build-pipeline
  image: $DOCKER_IMAGE
  services:
    - $DOCKER_IMAGE-dind
  script:
    - echo "Building docker image"
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build -t $PIPELINE_IMAGE_TAG -f Dockerfile.pipeline .
    - docker push $PIPELINE_IMAGE_TAG
    - docker image tag $PIPELINE_IMAGE_TAG $PIPELINE_IMAGE_REF
    - docker push $PIPELINE_IMAGE_REF

train-model:
  extends: .pipeline-changes
  stage: run-pipeline
  # image: registry.gitlab.com/dmiasport/password-complexity/password-pipeline:master
  image: $PIPELINE_IMAGE_REF
  script:
    # config git
    - git remote set-url origin https://dmiasport:${CI_PUSH_TOKEN}@gitlab.com/${CI_PROJECT_PATH}.git
    - git config --global user.email "dmiasport@gmail.com"
    - git config --global user.name "Gitlab Runner"
    # checking if requirements.txt is up-to-date and updating if necessary
    - pip install pipenv
    - pipenv lock -r > requirements.txt
    - dvc pull --run-cache
    - dvc repro
    - dvc push --run-cache
    # bump semver version
    # TODO: do this after quality check
    # TODO: this will fail to work properly if there will be a non-related tag with letters
    # this a workaround that needs to be fixed
    - export VERSION=$(git tag | grep -o '[0-9]*' | sort -n | tail -1 | awk -F. -v OFS=. 'NF==1{print ++$NF}; NF>1{$NF=sprintf("%0*d", length($NF), ($NF+1)); print}')
    - echo $VERSION
    # push changes to git
    - git commit -am "model was retrained in CI, pushing changes, new version $VERSION" --allow-empty
    - git tag -a $VERSION -m "model was retrained in CI and tagged as new version $VERSION"
    - git push --follow-tags origin HEAD:$CI_COMMIT_REF_NAME
    # send metrics and new data/models to gitlab comments
    # - cat metrics.json >> report.md
    # - cml-send-comment report.md

run-pytest-on-api:
  extends: .api-changes
  stage: test-api
  image: $PYTHON_IMAGE
  script:
    - echo

build-api-docker-image:
  extends: .api-changes
  stage: build-api
  image: $DOCKER_IMAGE
  services:
    - $DOCKER_IMAGE-dind
  script:
    # we use dvc here to download last model image
    # TODO: store cache apk with gitlab to speed up installation
    # some magic to install dvc in alpine image follows:
    - apk add libressl-dev musl-dev libffi-dev gcc python3 py3-pip python3-dev git libgit2 libgit2-dev
    - pip install -U pip
    - pip install dvc[gs]
    # TODO: we download the latest artifact and can't deploy any other version with CI/CD now
    # to improve, we could use a specific tag stored in a file or something else
    # gitlab fetches only HEAD and dvc fails with an error: "dvc" unknown Git revision 'refs/remotes/origin/HEAD'
    # to avoid this we create a temporal branch
    - git checkout -b temp-branch
    - dvc get . model
    # building docker image, supplying model/ folder to image
    - echo "Building docker image"
    - docker build -f Dockerfile.app -t build-image:latest .
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker image tag build-image:latest $APP_IMAGE_TAG
    - docker image tag build-image:latest $APP_IMAGE_REF
    - docker push $APP_IMAGE_TAG
    - docker push $APP_IMAGE_REF

deploy-api-staging:
  # TODO: extends now is not working as before, as arrays don't merge. Fix this
  extends:
    - .heroku-changes
  stage: deploy-api
  image: node:15.4.0-buster
  script:
    - apt-get update -qy
    # install dpl tool for deploy
    - apt-get install -y ruby-dev
    - gem install dpl
    # install dvc to pull model from remote storage
    - apt-get install -y cmake
    - apt-get install -y python3-pip
    - pip3 install -U pip
    - pip3 install dvc[all]
    - dvc get . model
    # do the actual deploy
    - dpl --provider=heroku --app=$HEROKU_STAGING_APP --api-key=$HEROKU_STAGING_API_KEY  --skip-cleanup
    # TODO: check model is up by sending request to the endpoint
    - curl $HEROKU_STAGING_URL
  environment:
    name: staging
    url: $HEROKU_STAGING_URL

# more common approach would be to ssh to the server, git clone this repo and run docker-compose up
# but gitlab-runner provide easier way to do that, basically executing ssh and git clone by itself
deploy-api-production:
  tags:
    - shell
  # TODO: extends now is not working as before, as arrays don't merge. Fix this
  extends:
    - .infra-changes
  stage: deploy-api
  script:
    - docker-compose up --build --detach
  environment:
    name: production
    url: $PRODUCTION_URL
