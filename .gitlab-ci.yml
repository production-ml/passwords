stages:
  - test-pipeline
  - build-pipeline
  - run-pipeline
  - test-api
  - build-api
  - deploy-api

# Change pip's cache directory to be inside the project directory since we can
# only cache local items.
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  HEROKU_STAGING_URL: https://$HEROKU_STAGING_APP.herokuapp.com/
  # tells pipenv where to keep virtualenvs
  WORKON_HOME: venv/
  # docker image details
  PYTHON_VERSION: 3.8.5
  PYTHON_IMAGE: python:${PYTHON_VERSION}-buster
  PIPELINE_IMAGE_NAME: password-pipeline
  PIPELINE_IMAGE_REF: $CI_REGISTRY_IMAGE/$PIPELINE_IMAGE_NAME:$CI_COMMIT_REF_SLUG
  PIPELINE_IMAGE_TAG: $CI_REGISTRY_IMAGE/$PIPELINE_IMAGE_NAME:$CI_COMMIT_SHA
  APP_IMAGE_NAME: password-app
  APP_IMAGE_REF: $CI_REGISTRY_IMAGE/$APP_IMAGE_NAME:$CI_COMMIT_REF_SLUG
  APP_IMAGE_TAG: $CI_REGISTRY_IMAGE/$APP_IMAGE_NAME:$CI_COMMIT_SHA

# Pip's cache doesn't store the python packages
# https://pip.pypa.io/en/stable/reference/pip_install/#caching
# If you want to also cache the installed packages, you have to install
# them in a virtualenv and cache it as well.
cache:
# untracked: true
 paths:
   - .cache/pip
   - venv/
   - .pyenv/cache

# define template which will execute jobs upon changes in ML pipeline
# because we unite package for ML pipeline and ML api,
# we need to carefully list here all files which changes shouldnt trigger CI/CD
# so it is a tradeoff of simplifying development at the cost of complicating CI/CD management
.pipeline-changes:
  only:
    changes:
      - package/lstm_model/**/*
      - scripts/**/*
      - Dockerfile.pipeline
      - Pipfile
      - Pipfile.lock
      - config.toml
  except:
    refs:
      - tags

# define template which will execute jobs upon changes in ML pipeline except schedules
# (because Gitlab treat all files as changed upon schedule CI/CD execution)
.pipeline-build:
  extends: .pipeline-changes
  except:
    refs:
      - schedules
      - tags

# define template which will execute jobs upon changes in ML API
.api-changes:
  rules:
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH
      changes:
      - package/**/*
    - if: '$CI_COMMIT_TAG != null'
# TODO: add except clauses
#   except:
#     refs:
#       - schedules
#     changes:
#       - package/lstm_model/**/*

.heroku-changes:
  rules:
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH
      changes:
      # "extends" is able to merge hashes but not arrays, thus we need to mention package again
      - package/**/*
      - .slugignore
      - Procfile
      - runtime.txt
    - if: '$CI_COMMIT_TAG != null'
# TODO: add except clauses
  # except:
  #   refs:
  #     - schedules

.infra-changes:
  rules:
    - if: $CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH
      changes:
      # "extends" is able to merge hashes but not arrays, thus we need to mention package again
      - package/**/*
      - config/**/*
    - if: '$CI_COMMIT_TAG != null'

run-pytest-on-pipeline:
  extends: .pipeline-build
  stage: test-pipeline
  image: $PYTHON_IMAGE
  script:
    - echo

build-pipeline-docker-image:
  extends: .pipeline-build
  stage: build-pipeline
  image: docker:19.03.12
  services:
    - docker:19.03.12-dind
  script:
    - echo "Building docker image"
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build -t $PIPELINE_IMAGE_TAG -f Dockerfile.pipeline .
    - docker push $PIPELINE_IMAGE_TAG
    - docker image tag $PIPELINE_IMAGE_TAG $PIPELINE_IMAGE_REF
    - docker push $PIPELINE_IMAGE_REF

train-model:
  extends: .pipeline-changes
  stage: run-pipeline
  # image: registry.gitlab.com/dmiasport/password-complexity/password-pipeline:master
  image: $PIPELINE_IMAGE_REF
  script:
    # config git
    - git remote set-url origin https://dmiasport:${CI_PUSH_TOKEN}@gitlab.com/${CI_PROJECT_PATH}.git
    - git config --global user.email "dmiasport@gmail.com"
    - git config --global user.name "Gitlab Runner"
    # checking if requirements.txt is up-to-date and updating if necessary
    - pip install pipenv
    - pipenv lock -r > requirements.txt
    # TODO: нужно добавить dvc в Pipfile, но там возникает какая-то проблема с tensorflow-cpu при pipenv lock
    - pip install dvc[all]
    - dvc pull
    - dvc repro
    - dvc push
    # bump semver version
    # TODO: do this after quality check
    - export VERSION=$(git describe --abbrev=0 | awk -F. -v OFS=. 'NF==1{print ++$NF}; NF>1{$NF=sprintf("%0*d", length($NF), ($NF+1)); print}')
    - echo $VERSION
    # push changes to git
    - git commit -am "model was retrained in CI, pushing changes, new version $VERSION" --allow-empty
    - git tag -a $VERSION -m "model was retrained in CI and tagged as new version $VERSION"
    - git push --follow-tags origin HEAD:$CI_COMMIT_REF_NAME
    # send metrics and new data/models to gitlab comments
    - cat metrics.json >> report.md
    - cml-send-comment report.md

run-pytest-on-api:
  extends: .api-changes
  stage: test-api
  image: $PYTHON_IMAGE
  script:
    - echo

build-api-docker-image:
  extends: .api-changes
  stage: build-api
  image: docker:19.03.12
  services:
    - docker:19.03.12-dind
  script:
    - echo "Building docker image"
    - docker build -f Dockerfile.app -t build-image:latest --build-arg GOOGLE_APPLICATION_CREDENTIALS .
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker image tag build-image:latest $APP_IMAGE_TAG
    - docker image tag build-image:latest $APP_IMAGE_REF
    - docker push $APP_IMAGE_TAG
    - docker push $APP_IMAGE_REF

deploy-api-staging:
  # TODO: extends now is not working as before, as arrays don't merge. Fix this
  extends:
    - .api-changes
    - .heroku-changes
  stage: deploy-api
  image: node:15.4.0-buster
  script:
    - apt-get update -qy
    - apt-get install -y ruby-dev
    - gem install dpl
    # do the actual deploy
    - dpl --provider=heroku --app=$HEROKU_STAGING_APP --api-key=$HEROKU_STAGING_API_KEY  # --skip-cleanup
    - curl $HEROKU_STAGING_URL
  environment:
    name: staging
    url: $HEROKU_STAGING_URL

# more common approach would be to ssh to the server, git clone this repo and run docker-compose up
# but gitlab-runner provide easier way to do that, basically executing ssh and git clone by itself
deploy-api-production:
  tags:
    - shell
  # TODO: extends now is not working as before, as arrays don't merge. Fix this
  extends:
    - .api-changes
    - .infra-changes
  stage: deploy-api
  script:
    - docker-compose up --build --detach
  environment:
    name: production
    url: $PRODUCTION_URL
